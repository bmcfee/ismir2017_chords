% -----------------------------------------------
% Template for ISMIR Papers
% 2017 version, based on previous ISMIR templates

% Requirements :
% * 6+n page length maximum
% * 4MB maximum file size
% * Copyright note must appear in the bottom left corner of first page
% * Clearer statement about citing own work in anonymized submission
% (see conference website for additional details)
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir,amsmath,cite,url}
\usepackage{graphicx}
\usepackage{color}


% Title.
% ------
\title{Structured training for large-vocabulary chord recognition}

% Note: Please do NOT use \thanks or a \footnote in any of the author markup

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
%  {First author} {School \\ Department}
%  {Second author} {Company \\ Address}

%% To make customize author list in Creative Common license, uncomment and customize the next line
%  \def\authorname{First Author, Second Author}


% Three addresses
% --------------
\threeauthors
  {First Author} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second Author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third Author} {Affiliation3 \\ {\tt author3@ismir.edu}}

%% To make customize author list in Creative Common license, uncomment and customize the next line
%  \def\authorname{First Author, Second Author, Third Author}

% Four or more addresses
% OR alternative format for large number of co-authors
% ------------
%\multauthor
%{First author$^1$ \hspace{1cm} Second author$^1$ \hspace{1cm} Third author$^2$} { \bfseries{Fourth author$^3$ \hspace{1cm} Fifth author$^2$ \hspace{1cm} Sixth author$^1$}\\
%  $^1$ Department of Computer Science, University , Country\\
%$^2$ International Laboratories, City, Country\\
%$^3$  Company, Address\\
%{\tt\small CorrespondenceAuthor@ismir.edu, PossibleOtherAuthor@ismir.edu}
%}
%\def\authorname{First author, Second author, Third author, Fourth author, Fifth author, Sixth author}


\sloppy % please retain sloppy command for improved formatting

\begin{document}

%
\maketitle
%
\begin{abstract}
The abstract should be placed at the top left column and should contain about 150-200 words.
\end{abstract}
%
\section{Introduction}\label{sec:introduction}

% Chord recognition is maturing as a problem within MIR

% The gains to be had are now in the large-vocab regime (eg, tetrads/sevenths)

% These classes are rare in the common datasets, so modeling them is hard

% But we can leverage the structure of chord space to better exploit available data

\subsection{Our contributions}

Training from structured representations

Deep recurrent chord model

Sampling strategy for training

Investigating data augmentation

%
\section{Related work}

% 2016 - idealized chroma prediction

% 2015 - four timely lessons

% 2013 - TMC (157)

% 2012 - HPA (bass tracking)

\section{Methods}

% Use convolutional filters for local representation

% Use a bidrectional recurrent model to capture dynamics

% Predict chord label from latent state representation

%% either directly

%% or by first estimating (root, pitch classes, bass)

\subsection{Encoder-decoder model}

% Figure for the basic model

% conv encoder, recurrent decoder model

% batch-norm on the cqt for standardization
\cite{ioffe2015batch}

% first filter = transient suppressor / local salience

% 3*12 full-height convolutional filters

% bidirectional recurrent model (GRU), d=128 in each direction
\cite{chung2014empirical}

% logistic to output vocabulary


\subsection{Chord vocabulary}

% For training the tag decoder, we map to a fixed vocabulary
%   1. discard missing / extra notes
%   2. discard inversions
%   3. split into (root, pitch classes)
%   4. match against quality templates:
%       - N
%       - maj, min, dim, aug
%       - min6, maj6
%       - min7, maj7, dom7, dim7, hdim7, minmaj7
%       - sus2, sus4
%       - X (unmatched)
%   5. resulting vocab = 12 * 14 + 2 = 170 classes
%

\cite{raffel2014mir_eval}


\subsection{Structured training}

% Figure for the structured model

% Note: for structured training, no simplification is performed
%       so that the model can still learn to map power chords to major, for example
%       but the decoder component is still trained to map to the constrained vocab

\cite{ni2012end} % HPA did bass tracking
\cite{cho2014improved} % k-stream HMM is a similar deal
\cite{korzeniowski2016feature} % idealized chroma -> logistic regression on small vocab

\subsection{Biased sampling}

% EJH used importance weighting per class
\cite{ejh2015four}

% we do it per track x
%   w(x) = 1/p^(x)
%   p^(x) := gmean_t p(y_t | x) = avg. probability of labels within the track
% sample patches uniformly from within tracks
% idea: down-weight tracks with low-entropy (compared to the corpus)
%   basically weighting by perplexity compared to the background distribution
%
% note: p(y) looks only at quality (after simplification), not root

\subsection{Data augmentation}

% MUDA
%   training set is augmented with pitch shifts of +- n semitones for n in {1,2,\dots, 6}
%   muda does annotation deformation as well
% all augmentations of a track get the same importance weight

\cite{mcfee2015software}

\section{Evaluation}

\subsection{Data}

% cite: ejh2015
%   1217, using the same 5-fold CV splits for comparison purposes
%   each training fold is split 75/25 for validation
%   training set => 12x by data augmentation
\cite{humphrey2015four}

% features
%   librosa 0.5
\cite{librosa050}
%   log cqt power, 36bpo, (C1 - C7) (260 bins)
%   sr=44100, hop = 4096 => ~96ms frame rate

% training setup
%   8sec patches (83 frames)
%   32 patches per batch
%   512 batches per epoch
%   ADAM optimization
\cite{kingma2014adam}
%   Keras + tensorflow
\cite{chollet2015keras, tensorflow2015-whitepaper}

%   validation by decoder loss
%   learning rate reduction after 10 epochs
%   early stopping after 20 epochs
%   maximum 100 epochs


\subsection{Results}

\section{Discussion}

\section*{Acknowledgments}

% For bibtex users:
\bibliography{refs}

\end{document}

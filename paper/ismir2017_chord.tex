% -----------------------------------------------
% Template for ISMIR Papers
% 2017 version, based on previous ISMIR templates

% Requirements :
% * 6+n page length maximum
% * 4MB maximum file size
% * Copyright note must appear in the bottom left corner of first page
% * Clearer statement about citing own work in anonymized submission
% (see conference website for additional details)
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir,amsmath,amssymb,cite,url}
\usepackage{graphicx}
\usepackage{color}
\usepackage{cleveref}
\usepackage{graphicx}
\usepackage{color}
\graphicspath{{figs/}}
\usepackage{booktabs}
\usepackage[inline]{enumitem}

\setlist[itemize]{noitemsep, topsep=0ex}
\setlist[enumerate]{noitemsep, topsep=0ex}

\def\eg{\emph{e.g.}}
\def\ie{\emph{i.e.}}
\def\etc{\emph{etc.}}
\def\etal{\emph{et al.}}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}


% Title.
% ------
\title{Structured training for large-vocabulary chord recognition}

% Note: Please do NOT use \thanks or a \footnote in any of the author markup

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
%  {First author} {School \\ Department}
%  {Second author} {Company \\ Address}

%% To make customize author list in Creative Common license, uncomment and customize the next line
%  \def\authorname{First Author, Second Author}


% Three addresses
% --------------
\twoauthors%
{Brian McFee\textsuperscript{1,2}} {\textsuperscript{1}Center for Data Science \\ New York University \\{\tt brian.mcfee@nyu.edu}}
{Juan Pablo Bello\textsuperscript{2}} {\textsuperscript{2}Music and Audio Research Laboratory \\ New York University \\ {\tt jpbello@nyu.edu}}

%% To make customize author list in Creative Common license, uncomment and customize the next line
%  \def\authorname{First Author, Second Author, Third Author}

% Four or more addresses
% OR alternative format for large number of co-authors
% ------------
%\multauthor
%{First author$^1$ \hspace{1cm} Second author$^1$ \hspace{1cm} Third author$^2$} { \bfseries{Fourth author$^3$ \hspace{1cm} Fifth author$^2$ \hspace{1cm} Sixth author$^1$}\\
%  $^1$ Department of Computer Science, University , Country\\
%$^2$ International Laboratories, City, Country\\
%$^3$  Company, Address\\
%{\tt\small CorrespondenceAuthor@ismir.edu, PossibleOtherAuthor@ismir.edu}
%}
%\def\authorname{First author, Second author, Third author, Fourth author, Fifth author, Sixth author}


\sloppy % please retain sloppy command for improved formatting

\begin{document}

%
\maketitle
%
\begin{abstract}
Automatic chord recognition systems operating in the large-vocabulary regime must overcome data scarcity: certain classes occur much less frequently than others, and this presents a significant challenge when estimating model parameters.
While most systems model the chord recognition task as a (multi-class) classification problem, few attempts have been made to directly exploit the intrinsic structural similarities between chord classes.

In this work, we develop a deep convolutional-recurrent model for automatic chord recognition over a vocabulary of 170 classes.
To exploit structural relationships between chord classes, the model is trained to produce both the time-varying chord label sequence as well as binary encodings of chord roots and qualities.
This binary encoding directly exposes similarities between related classes, allowing the model to learn a more coherent representation of simultaneous pitch content.
Evaluations on a corpus of 1217 annotated recordings demonstrate substantial improvements compared to previous models.
\end{abstract}
%
\section{Introduction}\label{sec:introduction}

% Chord recognition is maturing as a problem within MIR

% The gains to be had are now in the large-vocab regime (eg, tetrads/sevenths)

% These classes are rare in the common datasets, so modeling them is hard

% But we can leverage the structure of chord space to better exploit available data
Automatic chord recognition has been an active area of research within music informatics for nearly two decades~\cite{fujishima1999realtime}.
Chord recognition systems take as input an audio signal, and produce a time-varying symbolic representation of the signal in terms of \emph{chord labels}, which encode simultaneous pitch class content, such as \texttt{C:maj} or \texttt{G:hdim7}.
Many systems focus on simplified versions of this task, by predicting only the root note and \emph{major} or \emph{minor} qualities, or \emph{no-chord} (\texttt{N}).
Recently, interest has shifted toward the \emph{large-vocabulary} regime, where a broader class of chord qualities must be estimated, such as triads, sixths, sevenths, and suspended chords.

Typical chord recognition systems model the task as a time-varying multi-class classification problem.
This approach may be reasonable for the small-vocabulary regime, where the classes are sufficiently distinct to be modeled as unrelated, and each class may be observed with approximately uniform probability.
However, in the large-vocabulary setting, the multi-class formulation ignores the structural similarity between related chords, such as the shared notes between \texttt{C:min} and \texttt{C:min7}.
Moreover, the distribution of classes becomes highly skewed, thereby making it difficult to model these relationships from purely symbolic representations with no additional structure.
We hypothesize that leveraging known relationships between chord classes in terms of common roots and shared pitch classes can help mitigate the problem of observation bias, resulting in more accurate models of rare classes.

\subsection{Our Contributions}

We address the problem of large-vocabulary chord recognition by introducing a structured representation of chord qualities, which decouples the problem of detecting roots and pitch classes from the problem of mapping these properties onto symbolic labels.
We integrate this representation with deep, convolutional-recurrent neural networks, which are trained end-to-end to predict time-varying chord sequences from spectral audio representations.
The proposed models achieve substantially higher accuracy than previous models based on convolutional networks and hidden Markov models, resulting in absolute gains of 4--5\% in the most difficult categories (sevenths and tetrads).

%
\section{Related work}

Chord recognition has received a substantial amount of attention in the MIR literature, and a comprehensive survey of existing methods is beyond the scope of this paper.
Here, we highlight the work that is most closely related to the proposed methods in this paper.

%%% structured HMM methods
Hidden Markov models (HMMs) have been a popular method for designing chord recognition systems, and provide a flexible framework in which to integrate musical domain knowledge.
The general HMM approach models chord identities as latent state variables to be inferred from observed time-series features (\eg, chroma vectors).
Systems like Chordino~\cite{matthias2010a} and HPA~\cite{ni2012end} extend this idea by introducing additional latent variables to model key, bass, and metrical position.
In these systems, bass is modeled by weighting or partitioning the frequency range to produce distinct \emph{bass} and \emph{treble chroma} observations.
The $K$-stream HMM takes this idea a step further by modeling $K$ distinct frequency sub-bands, though it does not explicitly infer bass~\cite{cho2014improved}.
The structured representation we describe in \Cref{sec:methods} differs in that root, bass, and chord quality are jointly inferred from the entire spectrum, and it makes no assumptions about absolute height.
Weller~\etal~\cite{weller2009structured} also adapted structured training techniques for chord recognition, but at the level of dynamics rather than the chord vocabulary.

% 2010 - chordino, 159 but not quite the same
%   bass / treble tracking (pre-determined split)
%   no explicit root model
%   latent key model
%   metrical position + HMM dynamics
%   large vocab
%\cite{mauchsimple}

% 2012 - HPA (bass tracking), 25-class, 121-class
%   HPA did bass tracking
%   key, bass, root modeling
%   explicit partition of bass / treble by frequency
%   HMM for dynamics
%\cite{ni2012end}

% 2014 - TMC (157)
%   independent hmms per sub-band
%   
%\cite{cho2014improved} 

%%% Deep methods
In recent years, deep learning methods have been increasingly popular for chord recognition.
The majority of existing systems are trained in two stages. 
First, a model is built first to encode short patches of audio, \eg, as an idealized chroma vector~\cite{boulanger2013audio,korzeniowski2016feature} or likelihood distribution over chord categories~\cite{humphrey2015four,sigtia2015audio,zhou2015chord,deng2016hybrid}.
Second, a dynamics model integrates the time-series of learned representations to produce a sequence of predicted labels, \eg, using an HMM~\cite{humphrey2015four,zhou2015chord}, recurrent neural network (RNN)~\cite{boulanger2013audio,sigtia2015audio}, or simple point-wise prediction~\cite{korzeniowski2016feature}.
The models proposed here differ in that they are jointly trained end-to-end from spectral features, and learn the internal representation along with the dynamics using multiple recurrent layers.

Regardless of the model architecture, it is common to exploit some structural properties of chords, \eg, by tying model parameters for the same chord quality across roots~\cite{humphrey2015four}, or rotating chroma vectors through all possible root positions during training~\cite{cho2014improved}.
Although the methods we propose do not model quality independent of root, they do model active pitch class sets independently.
Chroma rotation can be viewed as a form of data augmentation, and the models we develop benefit substantially from a slightly more general form of augmentation described in \Cref{sec:muda}.
To the best of our knowledge, the proposed method is the first to exploit similarities between chords by jointly modeling labels and structured encodings.

% 2016 - idealized chroma prediction
%   cqt patch -> center frame idealized chroma
%   idealized chroma -> logistic regression on small vocab
%   not trained end-to-end
%   small vocab
%\cite{korzeniowski2016feature}

% 2013 
%   stft frame -> idealized chroma
%   idealized chroma -> rnn
%   supposedly large vocab, though no results
%\cite{boulanger2013audio}

% 2015 - four timely lessons (157)
%       cqt patch -> cnn -> center frame chord prediction
%       prediction sequence -> hmm
%       not trained end-to-end
%\cite{humphrey2015four}

% 2015 - sigtia, RNN
%       small vocab
%       cqt patch -> dnn -> center frame chord prediction
%       prediction sequence -> rnn
%       not trained end-to-end
%\cite{sigtia2015audio} 


%%% Deep but unstructured

% 2015 - zhou
%\cite{zhou2015chord}

% 2016 - deng
%   recurrent networks over very short patches
%   predict chord label of center frame
%   large vocab
%\cite{deng2016hybrid}



\section{Methods}
\label{sec:methods}
% Use convolutional filters for local representation

% Use a bidrectional recurrent model to capture dynamics

% Predict chord label from latent state representation

%% either directly

%% or by first estimating (root, pitch classes, bass)

This section outlines the data preparation, architectures, and training strategies for the models under comparison.
We consider three independent design choices: convolutional or recurrent decoding, the inclusion of structured output training, and the use of data augmentation.
This results in eight model configurations.

\subsection{Encoder-decoder models}

%The models architecture depicted in \Cref{fig:crnn1} 
%follows a encoder-decoder architecture.
The models we investigate fall under the umbrella of \emph{encoder-decoder} architectures~\cite{cho2015describing}.
The \emph{encoder} component maps time-varying input (audio) into a latent feature space, while the \emph{decoder} component maps from the latent feature space to the output space (chord labels).

\subsubsection{The encoder architecture}
The encoder, and depicted in \Cref{fig:encoder}, is common to all models considered in this paper.
Input audio is represented as a $T\times F$ time-series of log-power constant-Q transform (CQT) spectra (for $T$ frames and $F$ frequency bands).
After batch normalization~\cite{ioffe2015batch}, the first convolutional layer consists of a single two-dimensional $5\times5$ filter, followed by a bank of $36$ single-frame, one-dimensional convolutional filters, resulting in a $T\times 36$ feature map.
Both layers use rectified linear (ReLU) activations.
The first layer can be interpreted as a harmonic saliency enhancer, as it tends to learn to suppress transients and vibrato while emphasizing sustained tones.
The second layer summarizes the pitch content of each frame, and can be interpreted as a local feature extractor.

\begin{figure}[t]
    \centering
    %\includegraphics[width=\columnwidth]{crnn1}
    \includegraphics[width=\columnwidth]{encoder-r}
    \caption{The encoder module uses a convolutional-recurrent network architecture to map the input (CQT frames) to a sequence of hidden state vectors $h(t) \in \mathbb{R}^D$.\label{fig:encoder}}
\end{figure}

Finally, the local features are encoded by a bi-directional gated recurrent unit (GRU) model~\cite{cho2014learning}.
The GRU model is similar to the long-short-term memory (LSTM) model~\cite{hochreiter1997long}, but has fewer parameters and performs comparably in practice~\cite{jozefowicz2015empirical}.
For a sequence of $d$-dimensional input vectors $x(t) \in \mathbb{R}^d$, a GRU layer produces a sequence of $D$-dimensional hidden state vectors $h(t) \in {[-1, +1]}^D$ as follows:
\begin{eqnarray}
    r(t) &=& \sigma\left(W_r x(t) + T_r h(t-1) + b_r\right)\\
    u(t) &=& \sigma\left(W_u x(t) + T_u h(t-1) + b_u\right)\\
    \hat{h}(t) &=& \rho\left(W_h x(t) + T_h \left( r(t) \odot h(t-1) \right) + b_h \right)\label{eq:candidate}\\
    h(t) &=& u(t) \odot h(t-1) + (1-u(t)) \odot \hat{h}(t),
\end{eqnarray}
where $r(t), u(t) \in {[0,1]}^D$ are the \emph{reset} and \emph{update} vectors, each of which are controlled by RNN dynamics depending on the input $x(t)$ and previous hidden state $h(t-1)$, ${\sigma(x)={(1+\mathsf{e}^{-x})}^{-1}}$ denotes the logistic function, and $\rho = \tanh$.
The parameters are the input mappings $W_* \in \mathbb{R}^{D\times d}$, transition operators $T_* \in \mathbb{R}^{D\times D}$, and bias vectors $b_* \in \mathbb{R}^D$.

When an element $j$ of the update vector ${u_j(t)} \approx 1$, the corresponding element of the previous hidden state is copied directly to the current state ${h_j(t)} \leftarrow {h_j(t-1)}$.
Otherwise, if $r(t) \approx 1$, then $h(t)$ evolves according to standard RNN dynamics.
However, when both $u(t), r(t) \approx 0$, the $h$ term in~\eqref{eq:candidate} goes to 0 and the update \emph{resets}, depending only on the input $x(t)$.
This allows the GRU model to persist a hidden state across arbitrarily long spans of time, and capture variable-length temporal dependencies.
These properties make the GRU model appealing for chord recognition, where dependencies may span long ranges (compared to frames), and are subject to sudden changes rather than gradual evolution.

The bi-directional variant consists of two independent GRUs, one running in each temporal direction, whose hidden state vectors are concatenated to produce the bi-directional hidden state vector $h(t)$.
This layer integrates over the entire input signal, and provides temporal smoothing and context for the encoded feature representation.

\subsubsection{Decoder architectures}
%We evaluate the following decoder architectures, which trade breadth (number of hidden states) for depth (number of layers):
%\begin{description}
%    \item[CR1] the encoder Bi-GRU layer with 512 total hidden state dimensions (256 for each direction);
%    \item[CR2] a stack of two Bi-GRU layers, each with 256 hidden state dimension (128 in each direction, per layer). The hidden state from the first layer is treated as input to the second.
%\end{description}
We investigate two models, depicted in \Cref{fig:architectures}, for decoding $h(t)$ to the sequence of chord labels $\hat{y}(t)$.
The first model, denoted \emph{CR1}, decodes each frame independently:
\begin{equation}
    \hat{y}(t) := \text{softmax}\left(W_y h(t) + b_y\right),\label{eq:decode}
\end{equation}
where the soft-max operates over the chord vocabulary $V$, producing a likelihood vector $\hat{y}(t) \in {[0,1]}^{|V|}$.
For the \emph{CR1} architecture, we set the dimensionality of the hidden state vector to 512 (256 for each temporal direction).

The second model, denoted \emph{CR2}, uses a bi-GRU layer to map $h(t)$ to an intermediate representation $h_2(t)$ prior to frame-wise decoding by \cref{eq:decode}.
To keep the number of parameters roughly comparable between \emph{CR1} and \emph{CR2}, we set the dimensionality \emph{CR2}'s recurrent layers to 256.

For each configuration, all model parameters $\Theta$ are jointly trained to maximize the empirical log-likelihood:
\begin{equation}
    \argmax_\Theta    \sum_t \sum_{c \in V} y_c(t) \log \hat{y}_c(t), \label{objective}
\end{equation}
where the reference labels are one-hot encoded vectors $y(t) \in {\{0,1\}}^{|V|}$.
While both architectures have access to the entire observation sequence, \emph{CR2} may be better at capturing long-range interactions.
This should allow the encoder to focus on short-term smoothing and local context, while the decoder can model chord progressions and global context.
In the \emph{CR1} model, the encoder is responsible for both short- and long-range interactions.

At test time, the maximum likelihood label is selected for each frame, and the series of chord labels is run-length encoded to form the estimated annotation for the track.


\subsection{Chord vocabulary simplification}

\label{sec:vocab}
% For training the tag decoder, we map to a fixed vocabulary
%   1. discard missing / extra notes
%   2. discard inversions
%   3. split into (root, pitch classes)
%   4. match against quality templates:
%       - N
%       - maj, min, dim, aug
%       - min6, maj6
%       - min7, maj7, dom7, dim7, hdim7, minmaj7
%       - sus2, sus4
%       - X (unmatched)
%   5. resulting vocab = 12 * 14 + 2 = 170 classes
%

To formulate chord recognition as a classification task, we define a mapping of all valid chord descriptions to a finite vocabulary $V$.\footnote{A \emph{valid chord} is any string belonging to the formal language of Harte~\emph{et al.}~\cite{harte2005symbolic}, or the extended grammar implemented by JAMS~\cite{humphrey2014jams}.}
First, inversions and suppressed or additional notes are discarded, \eg:
\begin{align*}
    \texttt{D}\flat\texttt{:maj(9)/3} 
    \quad\mapsto\quad \texttt{D}\flat\texttt{:maj/3}
    \quad\mapsto\quad \texttt{D}\flat\texttt{:maj}.
\end{align*}
Next, labels are decomposed into \emph{root} and \emph{pitch classes} (relative to the root) using~\texttt{mir\_eval}~\cite{raffel2014mir_eval}:
\begin{align*}
    \texttt{D}\flat\texttt{:maj} &\mapsto \begin{cases}
        1 & \text{root}\\
        (0, 4, 7) & \text{pitch classes}
    \end{cases}.
\end{align*}
The set of active pitch classes is matched against 14 templates: \texttt{min, maj, dim, aug, min6, maj6, min7, minmaj7, maj7, 7, dim7, hdim7, sus2, sus4}.
The root and matched template are combined, and mapped to a canonical form to resolve enharmonic equivalences:
\begin{align*}
    \left(1, (0, 4, 7) \right) &\mapsto \texttt{C}\sharp\texttt{:maj}.
\end{align*}
If the pitch class set does not match one of the templates, it is mapped to the unknown chord symbol \texttt{X}; the no-chord symbol is represented distinctly as \texttt{N}.
The final vocabulary contains 170 classes: 2 special symbols (\texttt{N, X}), and $12\times14=168$ combinations of root and quality.


\subsection{Structured training}
\label{sec:encoding}

The CR models described above map each hidden state vector $h(t)$ to a fixed vocabulary produced described in \Cref{sec:vocab}.
They can be optimized in the usual way to maximize~\eqref{objective}, but this approach has some clear drawbacks.

First, it does not leverage the inherent structure of the space of chords.
If the model predicts \texttt{B:maj} instead of \texttt{B:7}, it is penalized just as badly as if it had predicted \texttt{C:maj}.
This is at odds with evaluation, where predictions are evaluated along multiple dimensions, such as capturing the root, third, or fifth.
More generally, some mistakes are simply more severe than others, and this is not reflected in a 1-of-K classification formulation.

Second, the chord simplification strategy is \emph{lossy} in that it discards information such as suppressed or additional notes.
This can render certain chords ambiguous, and can introduce discrepancies between the (simplified) annotation and the corresponding acoustic content.
Continuing the \texttt{D}$\flat$\texttt{:maj{(9)}/3} example, the simplification \texttt{C}$\sharp$\texttt{:maj} implies the absence of \texttt{D}$\sharp$, although it was explicitly included in the original annotation and should be expected in the signal.
This introduces label noise to the model, and may negatively impact accuracy.

Third, out-of-gamut chords all map to a common class \texttt{X}, despite having disparate roots and tonal content.
This class provides little useful information to the model while training.
At test time, it would be beneficial if the model could predict ``nearby'' chords, but multi-class training provides little incentive to learn this behavior.


To counteract these effects, we introduce a structured representation, depicted in \Cref{fig:encoding}.
This is inspired by the standard evaluation criteria for chord recognition, which operate over a decomposed representation of (\emph{root}, \emph{pitch classes}, \emph{bass})~\cite{raffel2014mir_eval}.
This representation can be computed for any valid chord label, and provided as supervision to the model, thereby helping it learn common features shared by similar chords.
At prediction time, the structured representation is used as an intermediate representation which contributes to the chord label prediction, which can now be interpreted as a human-readable decoding of the structured representation.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\columnwidth]{encoding}
    \caption{Target chords are represented in both simplified canonical form (\Cref{sec:vocab}), and as binary vectors encoding the root, bass, and pitch classes (\Cref{sec:encoding}).
    The special symbols \texttt{N,X} map to an extra root/bass class \texttt{N}, and the all-zeros pitch vector.\label{fig:encoding}}
\end{figure}

The structured models (denoted as \emph{CR1/2+S}), depicted in \Cref{fig:architectures}, predict for each frame $t$ the root pitch class (\texttt{C}--\texttt{B}, plus \texttt{N} for no-root), the bass pitch class, and the active pitch classes from the hidden state vector $h(t)$.
Root and bass estimation are modeled as a multi-class prediction with a soft-max non-linearity.
Pitch class prediction is modeled as a multi-label prediction, and uses a logistic (sigmoid) non-linearity.
This results in an idealized chroma representation similar to that of Korzeniowski and Widmer~\cite{korzeniowski2016feature}, but estimated from the full input observation rather than a fixed spectrogram patch.
An illustrative example of this predicted encoding is provided in \Cref{fig:encviz}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\columnwidth]{architectures}
    \caption{Block diagrams of all architectures described in \Cref{sec:methods}.
    The \emph{encoder} block is depicted in \Cref{fig:encoder}.\label{fig:architectures}}
\end{figure}

It is generally non-trivial to invert the root-pitch-bass representation to a unique chord label.
Therefore, these three layers are concatenated, along with the hidden state $h(t)$, to produce the structured representation from which the chord label is predicted.
During training, the structured models learn to minimize the sum of losses across all outputs: root, pitches, bass, and label $\hat{y}(t)$.
%In this way, the model can both leverage the structure of chord space, and learn how to decode the representation into the simplified vocabulary.
Minimizing the \emph{root} and \emph{pitches} losses corresponds to maximizing the \emph{root} and \emph{tetrads} recall scores during training, while \cref{objective} learns the decoding into the human-readable chord vocabulary.
This formulation effectively decouples the problems of root and pitch class identification from chord annotation, which is known to be subjective~\cite{humphrey2015four}.
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{encviz}
    \caption{The predicted chord encodings and labels for \emph{The Beatles --- Hold Me Tight} by model \emph{CR2+S+A}.\label{fig:encviz}}
\end{figure}
%A summary of all model architectures is provided in \Cref{tab:models}.

%\begin{table}[t]
%    \centering
%    \caption{Summary of model depth and size.\label{tab:models}}
%    \begin{tabular}{lrr}
%        \toprule
%        Model   & Layers & Parameters\\
%        \midrule
%        CR1     & 5 & 545,098\\
%        CR2     & 6 & 473,930\\
%        CR1+S   & 8 & 571,052\\
%        CR2+S   & 9 & 490,156\\
%        \bottomrule
%    \end{tabular}
%\end{table}


\subsection{Data augmentation}
\label{sec:muda}
To increase training set variability, we apply pitch-shifting data augmentation using MUDA~\cite{mcfee2015software}.
For each training example, 12 deformations are generated by shifting up or down by between 1--6 semitones.
Because each observation exists in all twelve root classes, this provides a brute-force, approximate root invariance to the model.
Models trained with data augmentation are denoted by \emph{+A}.


\section{Evaluation}
% cite: ejh2015
%   1217, using the same 5-fold CV splits for comparison purposes
%   each training fold is split 75/25 for validation
%   training set => 12x by data augmentation
For evaluation, we used the dataset provided by Humphrey and Bello~\cite{humphrey2015four}, which includes 1217 tracks from the Isophonics, Billboard, RWC Pop, and MARL collections.
To facilitate comparison with previous work, we retain the same 5-fold cross-validation splits, and randomly hold out 1/4 of each training set for validation.
We compare to two strong baselines: a deep convolutional network~\cite{humphrey2015four} (denoted \emph{DNN}), and the K-stream HMM~\cite{cho2014improved} (\emph{KHMM}).\footnote{Comparisons were facilitated using the pre-computed outputs provided at \url{https://github.com/ejhumphrey/ace-lessons}.}

\subsection{Pre-processing}

% features
%   librosa 0.5
Feature extraction was performed with librosa 0.5.0~\cite{librosa050}.
%   log cqt power, 36bpo, (C1 - C7) (260 bins)
Each track was represented as a log-power constant-Q spectrogram with 36 bins per octave, spanning 6 octaves starting at C1, and clipped at 80dB below the peak.
%   sr=44100, hop = 4096 => ~96ms frame rate
Signals were analyzed at 44.1KHz with a hop length of 4096 samples, resulting in a frame rate of approximately 10.8Hz.

\subsection{Training}
% training setup
All models are trained on 8-second patches (86 frames), though they readily support input of arbitrary length.
For tracks with multiple reference annotations, the output is selected uniformly at random from all references for the patch, which reduces sampling bias toward specific annotators.
%   8sec patches (83 frames)
%   32 patches per batch
%   512 batches per epoch
Models are trained using mini-batches of 32 patches per batch, and 512 batches per epoch.
%   ADAM optimization
We use the ADAM optimizer~\cite{kingma2014adam}, and reduce the learning rate if there is no improvement in validation score after 10 epochs.
Training is stopped early if there is no improvement in validation score after 20 epochs, and limited to a maximum of 100 epochs total.
For all models, validation score is determined solely by label likelihood (\cref{objective}).
%   validation by decoder loss
%   learning rate reduction after 10 epochs
%   early stopping after 20 epochs
%   maximum 100 epochs
%   Keras + tensorflow
All models were implemented with Keras~2.0 and Tensorflow~1.0~\cite{chollet2015keras, tensorflow2015-whitepaper}.\footnote{Our implementation is available at \url{https://github.com/bmcfee/ismir2017_chords}.}

\subsection{Results}

The main results of the evaluation are listed in \Cref{fig:results}, which illustrates the median weighted recall scores achieved by each model.\footnote{The trends for the mean scores are qualitatively similar, but the scores are lower for all models. We report the median here to reduce the influence of the erroneous or otherwise spurious reference annotations reported by Humphrey and Bello~\cite{humphrey2015four}.}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{crnn-scores}
    \caption{Weighted recall scores for all methods under comparison.  Each dot represents the median score across all test points, with error bars covering the 95\% confidence interval estimated by bootstrap sampling.
        \emph{KHMM} denotes the K-stream HMM of Cho~\cite{cho2014improved}; \emph{DNN} denotes the convolutional network of Humphrey and Bello~\cite{humphrey2015four}.\label{fig:results}}
\end{figure*}
Each subplot reports the recall scores computed by \texttt{mir\_eval}:
\begin{enumerate*}
    \item \emph{root};
    \item \emph{thirds}: root and third;
    \item \emph{triads}: root, third, and fifth;
    \item \emph{sevenths}: root, third, fifth, and seventh;
    \item \emph{tetrads}: all intervals;
    \item \emph{maj-min}: 12 major, 12 minor, and \texttt{N} class; and
    \item \emph{MIREX}: at least three correct notes.
\end{enumerate*}

From \Cref{fig:results}, several trends can be observed.
First, data augmentation (\emph{+A} variants) provides a consistent and substantial improvement for all models.
This is to be expected, since the \emph{CR} models do not separate root from quality.
Note that DNN models these independently, and KHMM was trained with chroma-rotation data augmentation, so it is unsurprising that augmentation is necessary to match performance of these methods.

Second, structured training (\emph{+S} variants) provides a modest, but consistent improvement, for both the shallow \emph{CR1} and deep \emph{CR2} decoder models.
The difference is most pronounced in the \emph{root} evaluation, which is expected due to the explicit objective to correctly identify the root.

Third, the deep decoder models \emph{CR2} provide another small, but consistent improvement over the shallow decoders \emph{CR1}.
The aggregate scores are reported in \Cref{tab:results}; for brevity, only the models with data augmentation are included.
The combined effect of structured training, deep decoder, and data augmentation (\emph{CR2+S+A}) results in the highest scoring model across all metrics.

\begin{table*}
    \centering
    \small
    \begin{tabular}{lrrrrrrr}
        \toprule
        Method  & Root & Thirds & Triads & Sevenths & Tetrads & Maj-Min & MIREX\\
        \midrule
        CR2+S+A & 0.861 & 0.836 & 0.812 & 0.729 & 0.671 & 0.855 & 0.852\\
        CR2+A   & 0.850 & 0.828 & 0.801 & 0.719 & 0.659 & 0.845 & 0.837\\
        CR1+S+A & 0.850 & 0.824 & 0.801 & 0.716 & 0.648 & 0.842 & 0.832\\
        CR1+A   & 0.841 & 0.815 & 0.791 & 0.702 & 0.647 & 0.834 & 0.829\\
        \midrule
        KHMM~\cite{cho2014improved}    & 0.849 & 0.822 & 0.785 & 0.674 & 0.629 & 0.817 & 0.827\\
        DNN~\cite{humphrey2015four}     & 0.838 & 0.809 & 0.766 & 0.654 & 0.605 & 0.803 & 0.812\\
        \bottomrule
    \end{tabular}
    \normalsize
    \caption{Median weighted recall scores for methods under comparison.\label{tab:results}}
\end{table*}
\subsection{Error analysis}

To get some more insight about the mistakes made by the model at test time, we illustrate the frame-wise, within-root quality confusion matrix for the \emph{CR2+S+A} model in \Cref{fig:confusion}.
For each frame of a test track, its (simplified) reference label is compared to the label estimated by the model if they match at the root.
Results are then aggregated across all test tracks, and normalized by (reference quality) frequency to produce the confusion matrix.
Under this evaluation, the \emph{CR2+S+A} achieves 63.6\% accuracy of correctly identifying the simplified chord label (root and quality) at the frame level.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{qualconf}
    \caption{Within-root, frame-wise quality confusions for the best performing model \emph{CR2+S+A}.
    The value at row $i$, column $j$ corresponds to the fraction of frames labeled as class $i$ but predicted as class $j$.\label{fig:confusion}}
\end{figure}

In \Cref{fig:confusion}, the first obvious trend is a bias toward \texttt{min} and \texttt{maj}, in accordance with the natural bias in the training set (13.6\% and 52.5\% of the data, respectively, by duration).
Note, however, that the confusions are generally understandable as simplifications: \eg, (\texttt{min7},\texttt{minmaj7})$\rightarrow$\texttt{min} and (\texttt{maj7},\texttt{7})$\rightarrow$\texttt{maj}.
The model still appears to struggle with 6th and suspended chords, which account for 1.5\% and 2.5\% of the data, respectively.
The bottom row corresponds to out-of-gamut \texttt{X}-chords, which map overwhelmingly to \texttt{maj} and \texttt{min}.
This can be explained by examining which labels map to \texttt{X} during simplification.
There are 4557 instances of such chords in the corpus (2.2\% of the data), and of these, 2091 are 1-chords (only the root) and 2365 are power chords (root+fifth), neither of which map unambiguously onto the simplified vocabulary.
The model appears to resolve these toward the more commonly used \texttt{min} and \texttt{maj} qualities.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{confdelta}
    \caption{The difference between confusion matrices for \emph{CR2+S+A} and the unstructured \emph{CR2+A} (best viewed in color).
    Positive values along the diagonal indicate increased accuracy for \emph{CR2+S+A}.\label{fig:confdelta}}
\end{figure}

To understand the influence of structured training, \Cref{fig:confdelta} illustrates the difference between the confusion matrices of the structured model \emph{CR2+S+A} and the unstructured model \emph{CR2+A}.
Positive values (red) along the diagonal indicate increased accuracy for the structured model, while negative values along the diagonal (blue) indicate decreased accuracy.
The net effect is positive, increasing accuracy by +0.8\% over \emph{CR2+A} (62.8\%).

Despite a slight degradation for \texttt{maj7}, there are substantial improvements for \texttt{aug}, \texttt{dim7}, \texttt{hdim7}, and modest improvement for \texttt{sus4}.
Moreover, the negative values in the second column reveal a consistent reduction of confusions to \emph{maj}.
This indicates that the structured model is more robust to quality bias in the training set.
Compared to the unstructured model, the structured model reduces confusions from \texttt{aug} to (\texttt{maj}, \texttt{7}), and \texttt{dim7} to (\texttt{min}, \texttt{7}, \texttt{N}).
The \emph{CR2+S+A} still performs poorly on the rarest class \texttt{minmaj7} (0.03\% of the data), but compared to \emph{CR2+A}, it resolves toward \texttt{min} more often and \texttt{min7} less often.
The structured model appears to be better at abstaining from predicting a seventh if it appears unlikely, rather than predict the wrong seventh.


\section{Conclusion}
This work developed deep architectures and a structured training framework for chord recognition in large vocabularies.
Although the proposed models improve over the baseline methods, there are clear directions forward in extending the ideas presented here.
First, although the proposed model predicts the bass note, this feature is only used for establishing context in decoding, and the model does not predict inversions.
Supporting inversion prediction would be a simple extension of the method described here, and would not require creating special vocabulary entries for each potential inversion.
Second, the structured representation facilitates modeling infrequently observed, complex chords, and could readily be extended to support extended chords by using a multi-octave pitch class representation.
However, doing so effectively---and evaluating the resulting predictions---would require larger annotated corpora for these classes than are presently available.

%   future work
%       expanded vocab
%           1/5 chords
%           extensions above the octave --> two-octave ideal chroma
%           this will require larger annotated corpora for these categories to be worthwhile though
%       bass tracking
%       


\section{Acknowledgments}
BM acknowledges support from the Moore-Sloan data science environment at NYU.
We thank the NVIDIA Corporation for the donation of a Tesla K40 GPU.

% For bibtex users:
\bibliography{refs}

\end{document}
